# -*- coding: utf-8 -*-
"""exp3.ipnyb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iCj1XK52fgENiUVjZWaXI3sTAt-d9SJQ
"""

!pip install torch torchvision
!pip install matplotlib
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

class Generator(nn.Module):
  def __init__(self, z_dim):
    super(Generator,self).__init__()
    self.fc1=nn.Linear(z_dim,128)
    self.fc2=nn.Linear(128,256)
    self.fc3=nn.Linear(256,512)
    self.fc4=nn.Linear(512,1024)
    self.fc5=nn.Linear(1024,28*28)
    self.relu=nn.ReLU()
    self.tanh=nn.Tanh()

  def forward(self,z):
    # Fix: Correctly call self.relu and self.fc1
    x=self.relu(self.fc1(z))  # Removed extra self and added self. to fc1
    x=self.relu(self.fc2(x))
    x=self.relu(self.fc3(x))
    x=self.relu(self.fc4(x))
    x=self.tanh(self.fc5(x))
    return x.view(x.size(0),1,28,28)

class Critic(nn.Module):
  def __init__(self):
    super(Critic,self).__init__()
    self.fc1=nn.Linear(28*28,1024)
    self.fc2=nn.Linear(1024,512)
    self.fc3=nn.Linear(512,256)
    self.fc4=nn.Linear(256,1)
    self.leaky_relu=nn.LeakyReLU(0.2)

  def forward(self,x):
    x=x.view(x.size(0),-1)
    x=self.leaky_relu(self.fc1(x))
    x=self.leaky_relu(self.fc2(x))
    x=self.leaky_relu(self.fc3(x))
    x=self.fc4(x)
    return x # Modified: Removed the extra call to self.fc4

transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,),(0.5,))
])

train_dataset=torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=transform)
test_dataset=DataLoader(train_dataset,batch_size=64,shuffle=True)

def mussertein_loss(y_true, y_pred):
  return torch.mean(y_true*y_pred)

def compute_gradient_penalty(critic, real_images, fake_images, device, lambda_gp=10):
    batch_size, C, H, W = real_images.size()
    epsilon = torch.rand((batch_size, 1, 1, 1)).to(device)  # Random epsilon for interpolation
    interpolated_images = epsilon * real_images + (1 - epsilon) * fake_images  # Linear interpolation
    interpolated_images.requires_grad_(True)

    # Forward pass through critic (discriminator)
    interpolated_scores = critic(interpolated_images)

    # Compute gradients w.r.t. the interpolated images
    gradients = torch.autograd.grad(
        outputs=interpolated_scores,
        inputs=interpolated_images,
        grad_outputs=torch.ones_like(interpolated_scores),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]  # Gradients w.r.t. images

    # Compute the norm of the gradients
    gradient_norm = gradients.view(batch_size, -1).norm(2, dim=1)  # L2 norm of gradients
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()  # Gradient penalty formula

    return lambda_gp * gradient_penalty

z_dim = 100
lr = 0.0005
n_critic = 5
lambda_gp = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize models
generator = Generator(z_dim).to(device)
critic = Critic().to(device)

# Optimizers
optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))
optimizer_c = optim.Adam(critic.parameters(), lr=lr, betas=(0.5, 0.9))

# Training loop
epochs = 100
for epoch in range(epochs):
    for i, (real_images, _) in enumerate(test_dataset):
        real_images = real_images.to(device)

        # Update the critic for n_critic steps
        for _ in range(n_critic):
            z = torch.randn(real_images.size(0), z_dim).to(device)  # Latent vector for fake images
            fake_images = generator(z)

            # Get critic scores for real and fake images
            critic_real = critic(real_images)
            critic_fake = critic(fake_images.detach())  # Detach fake images to avoid gradients through the generator

            # Compute gradient penalty
            gradient_penalty = compute_gradient_penalty(critic, real_images, fake_images, device, lambda_gp)

            # Critic loss: Wasserstein loss + gradient penalty
            critic_loss = -torch.mean(critic_real) + torch.mean(critic_fake) + gradient_penalty

            optimizer_c.zero_grad()
            critic_loss.backward()
            optimizer_c.step()

        # Update the generator
        z = torch.randn(real_images.size(0), z_dim).to(device)  # Latent vector for fake images
        fake_images = generator(z)
        critic_fake = critic(fake_images)

        # Generator loss: Negative critic score for fake images
        generator_loss = -torch.mean(critic_fake)

        optimizer_g.zero_grad()
        generator_loss.backward()
        optimizer_g.step()

    print(f'Epoch [{epoch+1}/{epochs}], Critic Loss: {critic_loss.item():.4f}, Generator Loss: {generator_loss.item():.4f}')

    # Visualization (every 10 epochs)
    if epoch % 10 == 0:
        with torch.no_grad():
            test_z = torch.randn(64, z_dim).to(device)  # Latent vector for generating test images
            generated_images = generator(test_z)
            generated_images = generated_images.cpu().data  # Move to CPU for plotting
            generated_images = generated_images.view(64, 28, 28)  # Reshape to 28x28 images

            # Plot generated images
            plt.figure(figsize=(10, 10))
            for i in range(64):
                plt.subplot(8, 8, i + 1)
                plt.imshow(generated_images[i], cmap='gray')
                plt.axis('off')
            plt.show()